# LoRA: Low-Rank Adaptation of Large Language Models

$$\max_{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log ( p_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{<}{t}) )$$

- https://arxiv.org/abs/2106.09685
- https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive
- LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning.
- When adapting to a specific task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace.
  - Inspired by this, we hypothesize the updates to the weights also have a low “intrinsic rank” during adaptation.
$$\text{rank}(\Delta W_{n \times k}) \ll \min(n, k)$$
- Why does this make sense? Large models are trained to capture the general representation of their domain (language for LLMs, audio + language for models like Whisper, and vision for image generation models). These models capture a variety of features which allow them to be used for diverse tasks with reasonable zero-shot accuracy. However, when adapting such a model to a specific task or dataset, only a few features need to be emphasized or re-learnt. This means that the update matrix (ΔW) can be a low-rank matrix.
- The rank of a matrix is the dimension of the vector space generated by its columns, which is given by the number of linearly independent columns (or rows) in a given matrix. It can be proven that the number of independent columns (known as column rank) is always equal to the number of independent rows (called row rank).
  - Hence, for a matrix A with m rows and n columns (represented as A_mn), $rank(A) \leq \min (m, n)$
$$h=W_0 x+\Delta W x=W_0 x+B A x$$
- No Additional Inference Latency: The key functional difference is that our learned weights can be merged with the main weights during inference, thus not introducing any latency, which is not the case for the adapter layers.

## Further questions
1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt to maximize downstream performance?
   - <img width="824" alt="image" src="https://github.com/junuMoon/review/assets/52732827/59588737-79e4-423c-91b8-fc4eebe7fca3">
   - This suggests that even a rank of four captures enough information in ∆W such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.
2) Is the “optimal” adaptation matrix ∆W really rank-deficient? If so, what is a good rank to use in practice?
   - Rank-Deficient: rank(∆W_nxk) << min(n, k)
   - Directions corresponding to the top singular vector overlap significantly between Ar=8 and Ar=64, while others do not. Specifically, ∆Wv (resp. ∆Wq) of Ar=8 and ∆Wv (resp. ∆Wq) of Ar=64 share a subspace of dimension 1 with normalized similarity > 0.5, providing an explanation of why r = 1 performs quite well in our downstream tasks for GPT-3.
3) What is the connection between ∆W and W? Does ∆W highly correlate with W? How large is ∆W comparing to W?
   - Second, instead of repeating the top singular directions of W, ∆W only amplifies directions that are not emphasized in W. Third, the amplification factor is rather huge: 21.5 ≈ 6.91/0.32 for r = 4. See Section H.4 for why r = 64 has a smaller amplification factor.
   - This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.

   
